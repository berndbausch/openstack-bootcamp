######################################################################
                       Kolla VM setup
######################################################################

Terminology: "host" is the physical machine where everything runs. 
The host is also the Kolla deployer.
"servers" are the controllers and compute nodes on which Kolla runs, 
plus another node for Elasticsearch.

For three controllers and two compute nodes, the host needs 64GB RAM. 
For a single controller and two compute nodes, 32GB should be OK.
The instructions are for the full setup but can easily be tuned down to
a single controller configuration.

- Three libvirt NAT networks named default, provider and lbmgmt.
  XML files for provider and lbmgmt exist.
- Each server has 2 CPUs, controllers 12 GB RAM, computes 8GB RAM (or more)
- 3 NICs connected to the three different NAT networks:
  ens3: API, management, VM tunnels, Swift, Cinder iSCSI, etc
  ens4: Flat provider network for VM's external access
  ens5: Flat provider network for loadbalancer management
- One root disk 50GB
- Three Swift disks, 50GB each
- One Cinder disk, 50GB each
- Controllers are named k1 k2 k3, Compute nodes c4 c5
  IP addresses on ens3 192.168.122.20{1..5} and on ens5 192.168.200.20{1..5}
  No IP address on ens4
- A separate node running Elasticsearch needs a single NIC with address 209.

######################################################################
                      Create the Kolla VMs' base disk
######################################################################

1- Install Ubuntu 20.04 on a disk named ubuntu-base.qcow2.
   Use make-ubuntu.sh to launch the install.
   Configure IP address 192.168.122.201 on the first NIC
   Name the user "stack", password "pw".
   No LVM. Install OpenSSH server. Install nothing else.

2- Customize the Ubuntu VM.
   If you do this right, you will have less work installing the servers.
   - Use visudo to configure password-less sudo.
   - Set the hostname to k1. Adjust /etc/hosts.
   - Edit /etc/netplan/00...yaml. Replace the first NIC with ens3.
     This is required because the Ubuntu machine uses virtio, but the 
     ultimate servers use E1000 as the NIC model. 
     You can change that by editing make-servers.sh, but you will be 
     on your own then. Do it if you know what you are doing.
   - Configure one further NICs for the external network:
       ens4:
         addresses: []
   - Use ssh-copy-id to add your public SSH key to the stack user.   

3. Power the Ubuntu machine off and make its disk immutable.
   The only purpose of the disk is to serve as a backup disk for
   the servers' qcow2 root disks.
   
   sudo chmod a+r ubuntu-base.qcow2
   sudo chattr +i ubuntu-base.qcow2

######################################################################
                      Create the Kolla VMs
######################################################################

NOTE: The make-*.sh scripts hard-code VNC ports. Check if the ports
need to change to avoid conflicts in your environment.

1- Controllers named k1, k2 and k3:
   for i in 1 2 3; do bash make-servers.sh $i; done

2- Compute nodes named c4 and c5:
   for i in 4 5; do bash make-computes.sh $i; done

3- Elasticsearch node:
   bash make-elastic.sh

Result: Six VMs, all with IP address 192.168.122.201.
Next step is to change the servers' netplan configs and hostnames. 
If you configured the original Ubuntu VM right, it has passwordless sudo 
for user stack and allows passwordless login to user stack.

This can be scripted but is currently manual. The process is: 
- Power all servers down except for k2.
- Change its netplan and hostname. Ping its new IP for testing.
- Optionally, create unique SSH host keys
- Power up the next server and so on. 
  Don't forget the Elastic server with IP address 209.
- When done, power up k1. Set its hostname to k1.

To change the netplan and hostname, where X is the correct IP:

    sed s:ADDR:20X: netplan.yaml | ssh k1 sudo tee /etc/netplan/00*yaml
    ssh k1 sudo hostnamectl set-hostname HOSTNAME
    ssh k1 sudo netplan apply
    ping 192.168.122.20X

The servers have identical SSH host keys. Unique SSH host keys are not 
really required, but for a more realistic setup, copy new-ssh-keys.sh 
to the servers and run it. After this, you may want to remove known_hosts 
on your host to avoid ugly error messages.

######################################################################
     Prepare controllers' Cinder and Swift disks
######################################################################
The compute nodes are done, but the controllers need an LVM VG for Cinder
and XFS filesystems with correct labels on the Swift disks.

for i in k1 k2 k3
do
    scp prepare-cinder.sh $i:
    scp prepare-swift.sh $i:
    ssh $i bash prepare-cinder.sh
    ssh $i bash prepare-swift.sh

Now the cloud nodes are done. Also install Elastic. You can do that now or
while Kolla is setting itself up.

######################################################################
     Install Elasticsearch
######################################################################

These instructions are verbatim from the elastic.co documentation on
installing Elasticsearch and Kibana on Debian. This works on Ubuntu, too.

1- Install the software
   You need to download the Elastic.co GPG keys, install apt-transport-https,
   add the Elastic.co repo and install.

   wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | 
   sudo apt-key add -

   sudo apt update && sudo apt-get install apt-transport-https

   echo "deb https://artifacts.elastic.co/packages/7.x/apt stable main" | 
   sudo tee /etc/apt/sources.list.d/elastic-7.x.list

   sudo apt-get update && 
   sudo apt-get install elasticsearch &&
   sudo apt install kibana

2- Configure Elasticsearch
   In /etc/elasticsearch/elasticsearch.yml:
      network.host: 192.168.122.209
      cluster.initial_master_nodes: ["192.168.122.209"]

3- Configure Kibana
   In /etc/kibana/kibana.yml:
      server.host: "192.168.122.209"
      elasticsearch.hosts: ["http://192.168.122.209:9200"]

4- System configuration

   cat <<EOF | sudo tee /etc/sysctl.d/99-elastic.conf
   vm.swappiness=1
   net.ipv4.tcp_retries2=5
   EOF
   sudo sysctl -p /etc/sysctl.d/99-elastic.conf

   Add this line to /etc/security/limits.conf: 
   *		 soft	 nofile		 1048576

sudo systemctl enable --now elasticsearch kibana

######################################################################
 On the host, install and configure Kolla-Ansible in a Python venv.
######################################################################

sudo apt update &&
sudo apt install python3-dev libffi-dev gcc libssl-dev python3-venv -y
# make venv
python3 -m venv ~/venv

# The echo ensures you enter the venv each time you log on.
echo source ~/venv/bin/activate >> .bashrc
source ~/venv/bin/activate

# update pip and install the correct version of Ansible
# For OpenStack Victoria, 2.9 is needed.
pip install -U pip
pip install 'ansible<2.10'

# install and configure Kolla-Ansible
pip install kolla-ansible
sudo mkdir -p /etc/kolla
sudo chown $USER:$USER /etc/kolla
cp -r ~/venv/share/kolla-ansible/etc_examples/kolla/* /etc/kolla

# Copy the inventory file from openstack-bootcamp/kolla3 to $HOME.
# Either multi-compute or single-controller.

# Ansible config
sudo mkdir /etc/ansible
cat <<EOF | sudo tee /etc/ansible/ansible.cfg
[defaults]
host_key_checking=False
pipelining=True
forks=100
log_path=$HOME/ansible.log
EOF

# generate passwords in /etc/kolla/passwords.yml
kolla-genpwd

# Feel free to adapt openstack-bootcamp/kolla3/globals.yml, 
# or use it unchanged.
# One detail that may require modification: Currently, DEBUG logs are
# configured. This can put too much load on the overall system.
cp $HOME/openstack-bootcamp/kolla3/globals.yml /etc/kolla

####################################################################
     Run a Docker registry on the deployer
####################################################################

https://docs.openstack.org/kolla-ansible/latest/user/multinode.html#option-2-registry-

First, install Docker on the deployer. 
https://docs.docker.com/engine/install/ubuntu/

Kolla-Ansible documents two Docker registry options. I opt for a mirror:
https://docs.openstack.org/kolla-ansible/latest/user/multinode.html#option-2-registry-mirror

sudo docker run -d --name registry --restart=always -p 4000:5000         \
                -v registry:/var/lib/registry                            \
                -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \
                registry:2

globals.yml requires: 

docker_custom_config:
  insecure-registries:
    - http://192.168.1.16:4000
  registry-mirrors:
    - http://192.168.1.16:4000

Non-standard port 4000 because Docker's 5000 is used by Keystone.
This will configure /etc/docker/daemon.json on all nodes.

####################################################################
     Create Swift rings on deployer
     This requires Docker on the deployer
####################################################################

bash prepare-swift-rings.sh

####################################################################
     Octavia certificates
####################################################################
Automatic generation documented at https://docs.openstack.org/kolla-ansible/latest/reference/networking/octavia.html#option-1-automatically-generating-certificates
This assumes certificate details are in globals.yml.

sudo chmod 777 /etc/kolla /etc/kolla/config      # may not be needed
kolla-ansible octavia-certificates

####################################################################
     Set up the cloud
####################################################################

# The bootstrap-servers step was done before creating Swift rings
kolla-ansible -i multi-compute bootstrap-servers
kolla-ansible -i multi-compute prechecks
kolla-ansible -i multi-compute deploy

The time this takes depends on the performance of your VMs and your 
internet bandwidth. Count 45 minutes at least.

In case of no errors, you now have a running Kolla cloud. A little bit
of post-deploy work is required.

####################################################################
      After deployment
####################################################################

# Install the openstack client on the deployer
# You may have to install other clients depending on your activities,
# e.g. python-gnocchiclient etc.
pip install python-openstackclient python-octaviaclient python-neutronclient osc-placement

# Generate an admin RC file 
kolla-ansible -i multi-compute post-deploy

# Script creates the Cirros image, external network "public1" and a 
# tenant network named "demo-net"

. /etc/kolla/admin-openrc.sh
export KOLLA_DEBUG=1
export ENABLE_EXT_NET=1
export EXT_NET_CIDR=192.168.100.0/24
export EXT_NET_RANGE=start=192.168.100.10,end=192.168.100.50
export EXT_NET_GATEWAY=192.168.100.1

./venv/share/kolla-ansible/init-runonce

# This script downloads a cirros image and registers it.  Then it configures
# networking and nova quotas to allow 40 m1.small instances to be created.

####################################################################
    Build Amphora image
####################################################################

https://docs.openstack.org/kolla-ansible/latest/reference/networking/octavia.html#octavia-amphora-image

sudo apt -y install debootstrap qemu-utils git kpartx
git clone https://opendev.org/openstack/octavia -b stable/victoria
pip install diskimage-builder
cd octavia/diskimage-create
./diskimage-create.sh

### Upload image to Glance

. /etc/kolla/octavia-openrc.sh
openstack image create amphora-x64-haproxy.qcow2 --container-format bare \
      --project service                                                  \
      --disk-format qcow2 --private --tag amphora --file amphora-x64-haproxy.qcow2 \
      --property hw_architecture='x86_64' --property hw_rng_model=virtio
# Image must be owned by the "service" project.
# Tag must match octavia_amp_image_tag in /etc/kolla/globals.yml. 
# Octavia uses the tag to determine which image to use. Default is "amphora". 

####################################################################
Useful software that should be added to deployer, controllers and computes
####################################################################

ansible -i multi-compute -m apt -a name=libvirt-clients compute
ansible -i multi-compute -m apt -a name=mariadb-client-10.3 control
sudo apt install mariadb-client    # on deployer

#######################################################################
#       PROBLEMS
#######################################################################

cinder-backup can't GET /info from Swift. May be due to incorrect 
setting of proxy-server expose_info setting, which should be true.
In the clouds that I created, GET /info only works when authenticated, but
cinder-backup assumes it works without authentication.

fluentd configuration doesn't seem to make central logs for most OpenStack 
services. The very last <match> clause in the td-agent.conf file is obviously
supposed to be a catch-all, sending all logs that have not yet been processed
to Elasticsearch. The match assumes a dot in the log source's tag, but earlier
in the config file, tags were renamed to be python-openstack - no dot included.
