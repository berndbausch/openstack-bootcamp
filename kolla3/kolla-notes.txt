######################################################################
                         VM setup
- 2 CPUs, 8GB RAM or more
- 2 NICs, same bridge OK?
- 50GB root
- Three Swift disks, 50GB
- One Cinder disk, 50GB
######################################################################
cd images-and-isos
qemu-img create -f qcow2 -b ubuntu20-base.qcow2 kolla.qcow2  # root disk
qemu-img create -f qcow2 kolla-cinder.qcow2 50
qemu-img create -f qcow2 kolla-swift1.qcow2 50
qemu-img create -f qcow2 kolla-swift2.qcow2 50
qemu-img create -f qcow2 kolla-swift3.qcow2 50

cd
virt-install --cpu host --name kolla --ram $((9*1024)) --vcpus 2 --graphics vnc,listen=0.0.0.0  \
             --network bridge=br1 --network bridge=br1    \
             --import                                     \
             --disk images-and-isos/kolla.qcow2           \
             --disk images-and-isos/kolla-swift1.qcow2    \
             --disk images-and-isos/kolla-swift2.qcow2    \
             --disk images-and-isos/kolla-swift3.qcow2    \
             --disk images-and-isos/kolla-cinder.qcow2    

######################################################################
     PREPARE
######################################################################

sudo apt update
sudo apt install python3-dev libffi-dev gcc libssl-dev -y
# make venv
sudo apt install python3-venv -y
python3 -m venv ~/venv

echo source ~/venv/bin/activate >> .bashrc
source ~/venv/bin/activate

# update pip and install the correct version of Ansible
pip install -U pip
pip install 'ansible<2.10'

# install and configure Kolla-Ansible
pip install kolla-ansible
sudo mkdir -p /etc/kolla
sudo chown $USER:$USER /etc/kolla
cp -r ~/venv/share/kolla-ansible/etc_examples/kolla/* /etc/kolla
cp ~/venv/share/kolla-ansible/ansible/inventory/* .

# Ansible config
sudo mkdir /etc/ansible
cat <<EOF | sudo tee /etc/ansible/ansible.cfg
[defaults]
host_key_checking=False
pipelining=True
forks=100
log_path=/home/stack/ansible.log
EOF

# create passwords in /etc/kolla/passwords.yml
kolla-genpwd

######################################################################
                      nodes' personalities
######################################################################
set IP addresses in netplan
hostnamectl set-hostname k[123].home
get /etc/hosts
sudo rm /etc/ssh/ssh_host*
sudo ssh-keygen -A   # create unique host keys

######################################################################
                     Add this to /etc/kolla/globals.yml
######################################################################
kolla_base_distro: "ubuntu"
openstack_logging_debug: "True"

network_interface: "ens3"
neutron_external_interface: "ens4"
kolla_internal_vip_address: "192.168.122.253"
enable_neutron_vpnaas: "yes"
enable_neutron_qos: "yes"
enable_neutron_provider_networks: "yes"
octavia_network_interface: "ens4"

enable_swift: "yes"
enable_cinder: "yes"
enable_cinder_backup: "yes"
enable_cinder_backend_lvm: "yes"
glance_backend_swift: "yes"
cinder_backup_driver: "swift"

enable_octavia: "yes"

octavia_certs_country: AT
octavia_certs_state: Tirol
octavia_certs_organization: Uni_Innsbruck
octavia_certs_organizational_unit: OpenStack

enable_central_logging: "yes"
enable_elastic_search_curator: "yes"

docker_custom_config:
  registry-mirrors:
    - http://CORRECT_URL:4000

####################################################################
             Check and/or explore other globals
####################################################################
enable_cells 
enable_neutron_provider_networks - why default "no"?
enable_mariabackup
enable_neutron_vpnaas
enable_neutron_dvr
enable_neutron_qos
enable_neutron_agent_ha
enable_redis
keystone_token_provider

####################################################################
       First step installing necessary software incl Docker
####################################################################

# Before this, create an identity and copy it from kolla-1 to 1 and 2
ssh-keygen
ssh-copy-id k1
ssh-copy-id k2
ssh-copy-id k3

Install multinode inventory

kolla-ansible -i multinode bootstrap-servers    

####################################################################
                   Create Swift rings
####################################################################

>>>> The commands below are in scripts.
>>>> On deployer: prepare-swift.sh
>>>> On the other controllers: prepare-swift-kolla2.sh

Swift requires manually setting up the disks and the rings
https://docs.openstack.org/kolla-ansible/latest/reference/storage/swift-guide.html
To understand the ring builder:
- https://docs.openstack.org/swift/latest/admin/objectstorage-ringbuilder.html
- https://docs.openstack.org/swift/latest/install/initial-rings.html

Steps:
1- create partitions on three disks, put XFS filesystems on them
2- use swift-ring-builder in Docker image to create object builder 
3- add the filesystems to the object ring
4- do all of the above for the account and container rings
5- rebalance

##### 1- create partitions on three disks, put XFS filesystems on them

index=0
for i in b c d
do 
    sudo parted -s /dev/sd$i mklabel gpt mkpart KOLLA_SWIFT_DATA 1 100%
    sudo mkfs -t xfs -L d${index} /dev/sd${i}1
    ((index++))
done

##### 2- use swift-ring-builder in Docker image to create object builder 
# https://docs.openstack.org/swift/latest/deployment_guide.html:
# swift-ring-builder <builder_file> create <part_power> <replicas> <min_part_hours>
# <min_part_hours> is the time in hours before a specific partition can be moved

KOLLA_SWIFT_BASE_IMAGE="kolla/ubuntu-source-swift-base:4.0.0"
mkdir -p /etc/kolla/config/swift
sudo docker run --rm -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
                $KOLLA_SWIFT_BASE_IMAGE swift-ring-builder \
                /etc/kolla/config/swift/object.builder create 10 3 1

##### 3- add the filesystems to the object ring
# swift-ring-builder <builder_file> add r<region>z<zone>-<ip>:<port>/<device_name>_<meta> <weight>
# It seems that labels are accepted as device names. Below: Labels d0, d1 and d2.

STORAGE_NODES=(192.168.1.208)
for node in ${STORAGE_NODES[@]}
do for i in {0..2}
   do sudo docker run --rm -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
      $KOLLA_SWIFT_BASE_IMAGE swift-ring-builder                                \
      /etc/kolla/config/swift/object.builder add r1z1-${node}:6000/d${i} 1
   done
done

##### 4- repeat steps 2 and 3 for the account and container rings

##### 5- rebalance
for ring in object account container 
do sudo docker run --rm -v /etc/kolla/config/swift/:/etc/kolla/config/swift/ \
                   $KOLLA_SWIFT_BASE_IMAGE swift-ring-builder                \
                   /etc/kolla/config/swift/${ring}.builder rebalance
done

####################################################################
                   Cinder LVM setup
####################################################################
Create volume group named cinder-volumes

####################################################################
                    Octavia certificates
####################################################################
Automatic generation documented at https://docs.openstack.org/kolla-ansible/latest/reference/networking/octavia.html#option-1-automatically-generating-certificates
Need to:
   1- put certificate description in global.yml
         octavia_certs_country: AT
         octavia_certs_state: Tirol
         octavia_certs_organization: Uni_Innsbruck
         octavia_certs_organizational_unit: OpenStack
   2- sudo chmod 777 /etc/kolla /etc/kolla/config
      kolla-ansible octavia-certificates
**Problem**: I lack write permissions on /etc/kolla; it's owned by kolla:kolla,
with permissions 755. Setting them to 777 makes the command run through,
but I am not sure if this is the right approach.
In contrast to the documentation, certificates are created under
/etc/kolla/octavia-certificates.

####################################################################
             Multinode requires local Docker registry
####################################################################

This is useful because Docker limits me to 200 image downloads per 6 hours.
It's better to have all images cached locally.

I opt for a registry mirror running on the labserver: 
https://docs.openstack.org/kolla-ansible/latest/user/multinode.html#option-2-registry-mirror

sudo docker run -d --name registry --restart=always -p 4000:5000         \
                -v registry:/var/lib/registry                            \
                -e REGISTRY_PROXY_REMOTEURL=https://registry-1.docker.io \
                registry:2

Add this to globals.yml: 

docker_custom_config:
  insecure-registries:
    - http://192.168.1.16:4000
  registry-mirrors:
    - http://192.168.1.16:4000

Non-standard port 4000 because Docker's 5000 is used by Keystone.
This will configure /etc/docker/daemon.json on all nodes.

####################################################################
                   Set up the cloud
####################################################################


# The bootstrap-servers step was done before creating Swift rings
# kolla-ansible -i multinode bootstrap-servers    # 5 minutes
kolla-ansible -i multi-compute prechecks
kolla-ansible -i multi-compute deploy

####################################################################
                   First steps in the cloud
####################################################################

pip install python-openstackclient   # doc says "python3-openstackclient"
pip install python-octaviaclient
pip install python-neutronclient
pip install osc-placement
kolla-ansible -i multi-compute post-deploy  # create adminrc with correct password
. /etc/kolla/admin-openrc.sh

# Script to set up a few initial resources

export KOLLA_DEBUG=1
export ENABLE_EXT_NET=1
export EXT_NET_CIDR=192.168.100.0/24
export EXT_NET_RANGE=start=192.168.100.10,end=192.168.100.50
export EXT_NET_GATEWAY=192.168.100.1

./venv/share/kolla-ansible/init-runonce

# This script downloads a cirros image and registers it.  Then it configures
# networking and nova quotas to allow 40 m1.small instances to be created.

####################################################################
Build Amphora image
####################################################################

It may exist already; check under ~stack/octavia/diskimage-create.

https://docs.openstack.org/kolla-ansible/latest/reference/networking/octavia.html#octavia-amphora-image

sudo apt -y install debootstrap qemu-utils git kpartx
git clone https://opendev.org/openstack/octavia -b stable/victoria
pip install diskimage-builder
cd octavia/diskimage-create
./diskimage-create.sh

### Upload image to Glance

. /etc/kolla/octavia-openrc.sh
openstack image create amphora-x64-haproxy.qcow2 --container-format bare \
      --project service                                                  \
      --disk-format qcow2 --private --tag amphora --file amphora-x64-haproxy.qcow2 \
      --property hw_architecture='x86_64' --property hw_rng_model=virtio
# Image must be owned by the "service" project.
# Tag must match octavia_amp_image_tag in /etc/kolla/globals.yml. 
# Octavia uses the tag to determine which image to use. Default is "amphora". 

####################################################################
Useful software that should be added
####################################################################

ansible -i multi-compute -m apt -a name=libvirt-clients compute
ansible -i multi-compute -m apt -a name=mariadb-client-10.3 control

### Configuration on elastic machine: 
sysctl  vm.swappiness=1
sysctl  net.ipv4.tcp_retries2=5
/etc/security/limits.conf: *		 soft	 nofile		 1048576

#######################################################################
#       PROBLEMS
#######################################################################

### None of the below suggested solutions work. Consider it work in progress.

lb-mgmt-net is isolated VXLAN 
--> transfer octavia-healthmanager port's MAC and IP to o-hm0 and
    plug o-hm0 into br-int. See Devstack.
MAC=port's MAC
PORT_ID=port's UUID
ovs-vsctl -- --may-exist add-port br-int o-hm0 -- set Interface o-hm0 type=internal -- set Interface o-hm0 external-ids:iface-status=active -- set Interface o-hm0 external-ids:attached-mac=$MAC -- set Interface o-hm0 external-ids:iface-id=$PORT_ID -- set Interface o-hm0 external-ids:skip_cleanup=true
sudo ip link set dev o-hm0 address $MAC
sudo iptables -I INPUT -i o-hm0 -p udp --dport 5555 -j ACCEPT
sudo iptables -I INPUT -i o-hm0 -p udp --dport 10514 -j ACCEPT
sudo iptables -I INPUT -i o-hm0 -p udp --dport 20514 -j ACCEPT


Or lb-mgmt-net is VLAN
--> need VLAN interface on labserver
-->    sudo ip link add link virbr1 name virbr1.100 type vlan id 100
-->    sudo ip link set virbr1.100 up
-->    sudo ip a del 192.168.100.1/24 dev virbr1
-->    sudo ip a add 192.168.100.1/24 dev virbr1.100
Do we also need an interface for VLAN 1000? (lb-mgmt-net)?

cinder-backup can't GET /info from Swift. May be due to incorrect 
setting of proxy-server expose_info (should be true).
GET /info should not require authentication.

Swift logs to UDP port 5410, but nobody listens. Swift logs go nowhere. 
It seems that fluentd is supposed to listen on 5410, but it doesn't.

